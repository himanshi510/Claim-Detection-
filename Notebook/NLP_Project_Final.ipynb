{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6LtaKHPO8t7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "df = pd.read_csv('F1_Claim_Detection_train.csv')\n",
        "# df = pd.read_csv('/content/aug_train.csv')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J_myzwKtdCk"
      },
      "outputs": [],
      "source": [
        "df_train = df.sort_values('label',axis = 0) #undersampling\n",
        "df_train = df.head(2000)\n",
        "df_train = df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GnXOT_LpTYJ"
      },
      "outputs": [],
      "source": [
        "#splitting training dataset for validation set\n",
        "df_val = df_train.head(int(len(df_train.index)*0.2))\n",
        "df_train = df_train.tail(int(len(df_train.index)*0.8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y43uV4vcN5NL"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npfkVw-cN8de"
      },
      "outputs": [],
      "source": [
        "def to_lower(str):\n",
        "  matches = re.compile(r'.').finditer(str)\n",
        "  ret_str = ''\n",
        "  for match in matches:\n",
        "    ret_str += str[match.start():match.end()].lower()\n",
        "  return ret_str\n",
        "\n",
        "\"\"\"def rem_punc(str):\n",
        "  str = re.sub(r'(\\.|\\?|\\,|!|\\:|\\;|\\&|\\-|\\(|\\)|\\{|\\}|\\'|\\\"|\\#|\\*|\\^|\\%|\\Â»)', ' ', str)\n",
        "  return str\"\"\"\n",
        "def rem_punc(str):\n",
        "  str = re.sub(r'[^a-z0-9A-Z\\s]', ' ', str)\n",
        "  return str\n",
        "\n",
        "def rem_url(str):\n",
        "  str = re.sub(r'(https?\\:\\/\\/)?(www\\.)?[A-Za-z0-9][A-Za-z0-9\\-]*(\\.(com|in|net|co|org|gov|ly|us|ac))+((\\/[A-Za-z0-9\\_]+)+\\.(php|aspx|html)|(\\/[A-Za-z0-9\\_]*)*)',' ', str)\n",
        "  return str\n",
        "\n",
        "def rem_username(str):\n",
        "    str = re.sub(r'(^|[\\s\\(\\)\\.\\,\\\\])\\@[a-zA-Z_0-9]+', ' ', str)\n",
        "    return str\n",
        "\n",
        "def rem_extraspace(str):\n",
        "  str = re.sub(r'\\s{2,}', ' ', str)\n",
        "  return str\n",
        "\n",
        "def rem_alphanum(str):\n",
        "    str = re.sub(r'(^|.)[0-9]+($|.)', '  ', str)\n",
        "    return str\n",
        "\"\"\"\n",
        "def spell_correct(arr):\n",
        "  spell_correct = Speller(lang = 'en')\n",
        "  for i in range(len(arr)):\n",
        "    arr[i] = spell_correct(arr[i])\"\"\"\n",
        "\n",
        "def rem_extra_alpha(str):\n",
        "    str = re.sub(r'([a-z])\\1{2,}', r'\\1\\1', str)\n",
        "    return str\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "def lemmatize(arr):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for i in range(len(arr)):\n",
        "    arr[i] = lemmatizer.lemmatize(arr[i])\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('quot')\n",
        "stop_words.add('amp')\n",
        "def rem_sw(arr):\n",
        "  global stop_words\n",
        "  reg_str = r'^'\n",
        "  for i in range(len(arr)):\n",
        "    for sw in stop_words:\n",
        "      reg_str+=sw\n",
        "      reg_str+='$'\n",
        "      arr[i] = re.sub(reg_str, '', arr[i])\n",
        "      reg_str = r'^'\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize(str):\n",
        "    a = word_tokenize(str)\n",
        "    return a\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "def lemmatize(arr):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for i in range(len(arr)):\n",
        "        arr[i] = lemmatizer.lemmatize(arr[i])\n",
        "    return arr\n",
        "def rem_empty(arr):\n",
        "    arr1 = [i for i in arr if i!='']\n",
        "    return arr1\n",
        "def preprocess(str):\n",
        "    str = to_lower(str)\n",
        "    str = rem_url(str)\n",
        "    str = rem_username(str)\n",
        "    str = rem_punc(str)\n",
        "    str = rem_alphanum(str)\n",
        "    str = rem_extra_alpha(str)\n",
        "    str = rem_extraspace(str)\n",
        "    #arr = tokenize(str)\n",
        "    #rem_sw(arr)\n",
        "    #spell_correct(arr)\n",
        "    #arr = rem_empty(arr)\n",
        "    #arr = lemmatize(arr)\n",
        "    return str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4yQ4ZaGORnR"
      },
      "outputs": [],
      "source": [
        "for i in range(len(df_train.index)):\n",
        "    p_str = preprocess(df_train.iloc[i,1])\n",
        "    df_train.at[i, 'tweet'] = p_str\n",
        "for i in range(len(df_val.index)):\n",
        "    p_str = preprocess(df_val.iloc[i,1])\n",
        "    df_val.at[i, 'tweet'] = p_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vjbRogv_kz5"
      },
      "source": [
        "POS TAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GwxDhp2Bwjs"
      },
      "outputs": [],
      "source": [
        "def pos_encoding(df):\n",
        "  sentags = [] #tags of all sentences\n",
        "  pstindata = [] #pos tags in data\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  result=[]\n",
        "  postags = ['CC', 'CD', 'DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','SYM','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB', '$', '#', '``', '(',')',',','.',':' ]\n",
        "  encodings=[]\n",
        "  for t in df['tweet']:\n",
        "    tagged = []\n",
        "    result = word_tokenize(t)\n",
        "    tagged = nltk.pos_tag(result)\n",
        "    sentags.append(tagged)\n",
        "    # for j in tagged:\n",
        "    #   if(j[1] not in pstindata):\n",
        "    #     pstindata.append(j[1])\n",
        "    map={}\n",
        "    encode=[]\n",
        "    for j in tagged:\n",
        "      if(j[1] in map):\n",
        "        map[j[1]]+=1       #create map as tag, occurence for this sentence\n",
        "      else:\n",
        "        map[j[1]]=1\n",
        "    for k in postags:\n",
        "      if k in map:\n",
        "        encode.append(map[k])\n",
        "      else:\n",
        "        encode.append(0)\n",
        "    encodings.append(encode)\n",
        "    for tags in map.keys():\n",
        "      if(tags not in postags):\n",
        "        print(tags, \"found\")\n",
        "  return encodings\n",
        "      #print(j[1])\n",
        "  # print(postags)\n",
        "  #print(len(encodings), len(encodings[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCqRauONImIQ"
      },
      "outputs": [],
      "source": [
        "postags = ['CC', 'CD', 'DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyFxJx-_BwWD"
      },
      "outputs": [],
      "source": [
        "print(nltk.pos_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlD7FyGTGgOf"
      },
      "outputs": [],
      "source": [
        "postags = ['CC', 'CD', 'DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXf_uSF_RAtj"
      },
      "source": [
        "NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEI5-I_JBVFI"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "#first counting the number of distinct ner tags present and then making dictionary so\n",
        "# for all sentences the encoding represents the same order\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "def ner_encoding(df):\n",
        "    ner_arr = []\n",
        "    for t in df['tweet']:\n",
        "        ner_arr.append(NER(t))\n",
        "    ner_encoding = []\n",
        "    ner_dict = {}\n",
        "    for ner_obj in ner_arr:\n",
        "        for ent in ner_obj.ents:\n",
        "            if not(ent.label_ in ner_dict.keys()):\n",
        "                ner_dict.update({ent.label_:len(ner_dict)})\n",
        "    import numpy as np\n",
        "    for i in range(len(ner_arr)):\n",
        "        temp = np.zeros((max(len(ner_dict),24),)) #there might be differences in\n",
        "        #number of ner tags, hence we fix it to 24\n",
        "        for ent in ner_arr[i].ents:\n",
        "            temp[ner_dict[ent.label_]]+=1\n",
        "        ner_encoding.append(temp)\n",
        "    return ner_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF6TrwgXX4Wl"
      },
      "source": [
        "DEPENDENCY PARSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERGrtee5poip"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install stanza\n",
        "import spacy\n",
        "import stanza\n",
        "from spacy import displacy\n",
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "stanza.download('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZLS4OqEpwrC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('F1_Claim_Detection_train.csv')\n",
        "dependency_parser = []\n",
        "encoding = []\n",
        "dictionaries = []\n",
        "j =0\n",
        "nlp = stanza.Pipeline('en', processors = \"tokenize,mwt, lemma ,pos,depparse\")\n",
        "# list_temp = [\"This is our NLP assignment\", \"Coronavirus was a world pandemic which caused deaths of many people.\"]\n",
        "for tweet in df[\"tweet\"]:\n",
        "    j+=1\n",
        "    #print(j)\n",
        "    encod = nlp(tweet)\n",
        "\n",
        "    store = encod.to_dict()\n",
        "\n",
        "    encoding_sentence = []\n",
        "    dictio = {}\n",
        "\n",
        "    words = len(store[0])\n",
        "\n",
        "    temp = []\n",
        "\n",
        "    for i in range(words):\n",
        "        dic_temp = store[0][i]\n",
        "        temp.append(dic_temp[\"upos\"])\n",
        "    \n",
        "    for ele in temp:\n",
        "        if ele in dictio:\n",
        "            dictio[ele] += 1\n",
        "        else:\n",
        "            dictio[ele] = 1\n",
        "\n",
        "    dictionaries.append(dictio)\n",
        "    # for key in dictio.keys():\n",
        "    #     encoding_sentence.append(dictio[key])\n",
        "\n",
        "\n",
        "    dependency_parser.append(encod)\n",
        "    \n",
        "\n",
        "tags = {}   # tags(dictionary) will contain all the possible upos values\n",
        "for dic in dictionaries:\n",
        "\n",
        "    for key in dic.keys():\n",
        "        if key not in tags:\n",
        "            tags[key] = 1\n",
        "tags_list = [] # tags_list(array) will contain all the possible upos values\n",
        "for key in tags.keys():\n",
        "    tags_list.append(key)\n",
        "\n",
        "for dic in dictionaries:\n",
        "    # Each dic contain information about the tweets\n",
        "    encoding_sentence = []\n",
        "    for tag in tags_list:\n",
        "        if tag in dic.keys():\n",
        "            encoding_sentence.append(dic[tag])\n",
        "        else:\n",
        "            encoding_sentence.append(0)\n",
        "    encoding.append(encoding_sentence)\n",
        "# print(encoding, len(tags_list))\n",
        "# print(len(tags))\n",
        "\n",
        "df_encode = pd.DataFrame(encoding, columns=[\"1\",'2','3','4','5','6','7','8','9', '10','11', '12', '13', '14', '15', '16', '17'])\n",
        "df_encode.to_csv('encodings_train.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga6_JYPUp-SH"
      },
      "outputs": [],
      "source": [
        "df_encode = pd.DataFrame(encoding, columns=[\"1\",'2','3','4','5','6','7','8','9', '10','11', '12', '13', '14', '15', '16', '17'])\n",
        "df_encode.to_csv('encodings_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcZ6BzvqqF8M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_test = pd.read_csv('F1_Claim_Detection_test.csv')\n",
        "dependency_parser_test = []\n",
        "encoding_test = []\n",
        "dictionaries_test = []\n",
        "j =0\n",
        "nlp = stanza.Pipeline('en', processors = \"tokenize,mwt, lemma ,pos,depparse\")\n",
        "# list_temp = [\"This is our NLP assignment\", \"Coronavirus was a world pandemic which caused deaths of many people.\"]\n",
        "for tweet in df_test[\"tweet\"]:\n",
        "    j+=1\n",
        "    print(j)\n",
        "    encod_test = nlp(tweet)\n",
        "\n",
        "    store = encod_test.to_dict()\n",
        "\n",
        "    encoding_sentence_test = []\n",
        "    dictio_test = {}\n",
        "\n",
        "    words = len(store[0])\n",
        "\n",
        "    temp = []\n",
        "\n",
        "    for i in range(words):\n",
        "        dic_temp = store[0][i]\n",
        "        temp.append(dic_temp[\"upos\"])\n",
        "    \n",
        "    for ele in temp:\n",
        "        if ele in dictio_test:\n",
        "            dictio_test[ele] += 1\n",
        "        else:\n",
        "            dictio_test[ele] = 1\n",
        "\n",
        "    dictionaries_test.append(dictio)\n",
        "    # for key in dictio.keys():\n",
        "    #     encoding_sentence.append(dictio[key])\n",
        "\n",
        "\n",
        "    dependency_parser_test.append(encod)\n",
        "    \n",
        "\n",
        "tags_test = {}   # tags(dictionary) will contain all the possible upos values\n",
        "for dic in dictionaries_test:\n",
        "\n",
        "    for key in dic.keys():\n",
        "        if key not in tags_test:\n",
        "            tags_test[key] = 1\n",
        "tags_list_test = [] # tags_list(array) will contain all the possible upos values\n",
        "for key in tags_test.keys():\n",
        "    tags_list_test.append(key)\n",
        "\n",
        "for dic in dictionaries_test:\n",
        "    # Each dic contain information about the tweets\n",
        "    encoding_sentence_test = []\n",
        "    for tag in tags_list_test:\n",
        "        if tag in dic.keys():\n",
        "            encoding_sentence_test.append(dic[tag])\n",
        "        else:\n",
        "            encoding_sentence_test.append(0)\n",
        "    encoding_test.append(encoding_sentence_test)\n",
        "# print(encoding, len(tags_list))\n",
        "# print(len(tags))\n",
        "\n",
        "df_encode_test = pd.DataFrame(encoding_test, columns=[\"1\",'2','3','4','5','6','7','8','9'])\n",
        "df_encode_test.to_csv('encodings_test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-4wqiHQ_3yl"
      },
      "source": [
        "GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJr9fYg7AJIv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/work/glove.6B.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/glove')\n",
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "f = open('/content/glove/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "def get_emb(tokenized_sent): #tokenized_sent is a 1d array\n",
        "    vec = np.zeros((384,))\n",
        "    count = 1\n",
        "    for i in tokenized_sent:\n",
        "        try:\n",
        "            vec+=embeddings_index[i]\n",
        "            count+=1\n",
        "            \n",
        "        except:\n",
        "            pass\n",
        "    vec/= count\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvX8nkuCrLfT"
      },
      "outputs": [],
      "source": [
        "glove_encoding_train = []\n",
        "for t in df_train['tweet']:\n",
        "    glove_encoding_train.append(get_emb(word_tokenize(t)))\n",
        "\"\"\"glove_encoding_val = []\n",
        "for t in df_val['tweet']:\n",
        "    glove_encoding_val.append(get_emb(word_tokenize(t)))\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W7gewfSeacO"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSH1w8HheZDj"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "\n",
        "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc-qPD03cjDB"
      },
      "outputs": [],
      "source": [
        "bert_emb_train = bert_model.encode(df_train['tweet'])\n",
        "#print(bert_emb_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK_zjafWt2PO"
      },
      "outputs": [],
      "source": [
        "pos_encoding_train = pos_encoding(df_train)\n",
        "ner_encoding_train = ner_encoding(df_train)\n",
        "final_encoding_train = np.hstack(( ner_encoding_train, bert_emb_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smWKDClRy_Pw"
      },
      "outputs": [],
      "source": [
        "y_train = df_train['label'].values\n",
        "y_val = df_train['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHMvyzdLzcEO"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "svm1 = SVC()\n",
        "svm1.fit(final_encoding_train, y_train)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(final_encoding_train, y_train)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=50, max_depth=10)\n",
        "rf.fit(final_encoding_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnklfhOMm9qh"
      },
      "outputs": [],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H9RQMgCaN7z"
      },
      "outputs": [],
      "source": [
        "pred_lr = lr.predict(final_encoding_val)\n",
        "pred_svm = svm1.predict(final_encoding_val)\n",
        "pred_rf = rf.predict(final_encoding_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0EvKG9b2Y_5"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('/content/F1_Claim_Detection_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8P600hP2jjc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(df_test.index)):\n",
        "    p_str = preprocess(df_test.iloc[i,0])\n",
        "    df_test.at[i, 'tweet'] = p_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm0U_9-i4v_e"
      },
      "outputs": [],
      "source": [
        "pos_encoding_test = pos_encoding(df_test)\n",
        "ner_encoding_test = ner_encoding(df_test)\n",
        "\"\"\"glove_encoding_test = []\n",
        "for t in df_test['tweet']:\n",
        "    glove_encoding_test.append(get_emb(t))\"\"\"\n",
        "bert_emb_test = bert_model.encode(df_test['tweet'])\n",
        "print(bert_emb_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gYx0F0G3sFo"
      },
      "outputs": [],
      "source": [
        "final_encoding_test = np.hstack((pos_encoding_test, ner_encoding_test, bert_emb_test))\n",
        "print(np.shape(final_encoding_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoePshZlcf6o"
      },
      "outputs": [],
      "source": [
        "pred_lr = lr.predict(final_encoding_test)\n",
        "pred_svm = svm1.predict(final_encoding_test)\n",
        "pred_rf = rf.predict(final_encoding_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwMd2k1tz6b_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(y_test, pred_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oitByaD4BtVO"
      },
      "outputs": [],
      "source": [
        "pred = lr.predict(final_encoding_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znI42yg3B1lM"
      },
      "outputs": [],
      "source": [
        "df_kaggle = pd.DataFrame(np.arange(0,len(pred)),columns = ['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaxzLuUwCLBf"
      },
      "outputs": [],
      "source": [
        "df_kaggle.insert(1, \"label\", np.asarray(pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m36yLF5VCyQW"
      },
      "outputs": [],
      "source": [
        "df_kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcVZ89LHDHP1"
      },
      "outputs": [],
      "source": [
        "df_kaggle.to_csv('/content/submission1.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVQsJBsOtPaG"
      },
      "source": [
        "Oversampling/ Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-fPfThSqh_3"
      },
      "outputs": [],
      "source": [
        "!pip install numpy requests nlpaug\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKYeckANrFbO"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "aug = naw.SynonymAug(aug_src='wordnet',aug_max=3)\n",
        "to_augment = df_train[df_train['label']==0]\n",
        "to_augmentX = to_augment['tweet']\n",
        "# to_augmentY = np.zeros(len(to_augmentX.index) * num_times, dtype=np.int8)\n",
        "sentences = []\n",
        "for j in to_augmentX:\n",
        "  x = aug.augment(j,n=5)\n",
        "  for sent in x:\n",
        "    sentences.append(sent)\n",
        "lbl=[]\n",
        "for k in range (len(sentences)):\n",
        "  lbl.append(0)\n",
        "\n",
        "df2 = pd.DataFrame({\"tweet\":sentences,\"label\":lbl})\n",
        "newdf_train = df_train.append(df2, ignore_index=True)\n",
        "print(newdf_train)\n",
        "newdf_train.to_csv('aug_train.csv', index=False,)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBwuw1WJtR9Z"
      },
      "source": [
        "Fine Tuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV6BcoESrI8p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBdHcPKxrRVs"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "from datasets import load_dataset, load_from_disk\n",
        "dataset = load_dataset('csv', data_files={'train': 'aug_train.csv'})\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")  #distilbert-base-uncased\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"tweet\"], padding=\"max_length\", truncation=True)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa9U5nanrVLg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "checkpoint = \"sentence-transformers/all-mpnet-base-v2\"   #distilbert-base-uncased\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoUkCVxIsqfC"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "num_train_epochs=3\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBE8LsvXswZ-"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(args=training_args,model=model,\n",
        "   train_dataset=tokenized_datasets[\"train\"] \n",
        ")\n",
        "trainer.train()\n",
        "model.save_pretrained(\"claim/finetunemodel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnB3biElruTg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model.save_pretrained(\"claim/finetunemodel\")\n",
        "# alternatively save the trainer\n",
        "# trainer.save_model(\"CustomModels/CustomHamSpam\")\n",
        "tokenizer.save_pretrained(\"claim/finetunemodel\")\n",
        "# load the model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "load_model = AutoModelForSequenceClassification.from_pretrained(\"claim/finetunemodel\")\n",
        "load_tokenizer = AutoTokenizer.from_pretrained(\"claim/finetunemodel\")\n",
        "model = load_model\n",
        "tokenizer = load_tokenizer\n",
        "my_pipe  = pipeline(\"text-classification\", model=load_model, tokenizer=load_tokenizer)\n",
        "data = list(df_test[\"tweet\"])\n",
        "\n",
        "\n",
        "ls = my_pipe(data)\n",
        "label_test=[]\n",
        "for j in ls:\n",
        "  if(j['label']=='LABEL_0'):\n",
        "    label_test.append(0)\n",
        "  elif(j['label']=='LABEL_1'):\n",
        "    label_test.append(1)\n",
        "  else:\n",
        "    print(\"error\")\n",
        "\n",
        "df_kaggle = pd.DataFrame(np.arange(0,len(label_test)),columns = ['id'])\n",
        "df_kaggle.insert(1, \"label\", np.asarray(label_test))\n",
        "df_kaggle.to_csv('submission1.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w3iv6vRcpKr"
      },
      "source": [
        "References for code:\n",
        "\n",
        "[Extracting pre-trained GloVe embeddings](https://blog.paperspace.com/pre-trained-word-embeddings-natural-language-processing/)\n",
        "\n",
        "[NER- Spacy](https://spacy.io/usage/linguistic-features#named-entities)\n",
        "\n",
        "[Text augmentation](https://towardsdatascience.com/powerful-text-augmentation-using-nlpaug-5851099b4e97)\n",
        "\n",
        "[Fine-tuning](https://levelup.gitconnected.com/how-to-fine-tune-an-nlp-classification-model-with-transformers-and-huggingface-1a2c0ea79c2)\n",
        "\n",
        "Some code reused from previous assignments"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
